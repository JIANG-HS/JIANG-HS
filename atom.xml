<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://JIANG-HS.github.io</id>
    <title>JIANG-HS</title>
    <updated>2020-07-05T02:31:10.049Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://JIANG-HS.github.io"/>
    <link rel="self" href="https://JIANG-HS.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://JIANG-HS.github.io/images/avatar.png</logo>
    <icon>https://JIANG-HS.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, JIANG-HS</rights>
    <entry>
        <title type="html"><![CDATA[K-means算法]]></title>
        <id>https://JIANG-HS.github.io/post/t51XZdxit/</id>
        <link href="https://JIANG-HS.github.io/post/t51XZdxit/">
        </link>
        <updated>2020-05-31T10:16:55.000Z</updated>
        <content type="html"><![CDATA[<h1 id="一-k-means介绍">一、K-means介绍</h1>
<p>K-means算法，也称为K-平均或者K-均值，是一种无监督的聚类算法。对于给定的样本集，按照样本之间的距离大小，将样本划分为K个簇，让簇内的点尽量紧密的连接在一起，而让簇间的距离尽量的大。K-means是一种使用广泛的最基础的聚类算法，通常作为学习聚类算法时的第一个算法。<br>
其他的聚类算法：K-medoids、k-modes、Clara、Clarans等</p>
<p><strong>聚类</strong>：物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类。由聚类所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，与其他簇中的对象相异。</p>
<p><strong>簇</strong>：本算法中可以理解为，把数据集聚类成n类，即为n个簇。</p>
<p><strong>欧几里得距离公式</strong>（也叫欧式距离）：<img src="https://JIANG-HS.github.io/post-images/1591346947075.jpg" alt="" loading="lazy"></p>
<h1 id="二-算法步骤">二、算法步骤</h1>
<p>1.给定一个待处理的数据集；<br>
2.记K个簇的中心分别为a<sub>1</sub>,a<sub>2</sub>,...,a<sub>k</sub>;每个簇的样本数量为N<sub>1</sub>,N<sub>2</sub>,...,N<sub>3</sub>;<br>
3.通过欧几里得距离公式计算各点到各质心的距离，把每个点划分给与其距离最近的质心，从而初步把数据集分为了K类；<br>
4.更新质心：通过下面的公式来更新每个质心。就是，新的质心的值等于当前该质心所属簇的所有点的平均值。<br>
<img src="https://JIANG-HS.github.io/post-images/1591346626448.jpg" alt="" loading="lazy"><br>
5.重复步骤3和步骤4，直到质心不再变化或者达到最大迭代次数。</p>
<h1 id="三-图形展示">三、图形展示</h1>
<p>假设K=2，即有两个簇，绿色为最初的样本数据集（图a），红色标记和蓝色标记分别为两个质心（图b）。通过计算样本到红色质心和蓝色质心的距离，实现对样本的分类，然后再不断地更新质心的位置，最终得到了一个比较理想的聚类结果（图f）。<br>
<img src="https://JIANG-HS.github.io/post-images/1591347084111.png" alt="" loading="lazy"><br>
顺序为：a→b→c→d→e→f<br>
可以看到，整个算法是一个不断更新质心和簇的过程。</p>
<h1 id="四-代码实现">四、代码实现</h1>
<pre><code>import matplotlib.pyplot as plt
from random import uniform#randint
import numpy as np
from math import sqrt

#创建一个数据集。注意：本代码的数据集是随机的
m=40
x = list(range(m))
y = list(range(m))
for i in range(m):
    if i &lt; m/2:
        x[i] = uniform(1,5)
        y[i] = uniform(1,5)
    else:
        x[i] = uniform(6,10)
        y[i] = uniform(6,10)
#将创建的数据集画成散点图
plt.scatter(x,y)
plt.xlim(0,11)
plt.ylim(0,11)
#plt.show()

#这里直接给定质心了，为了看起来更明显，当然随机是最好的
cent1 = [3,9]
cent2 = [8,3]
plt.scatter(cent1[0],cent1[1],marker='*')
plt.scatter(cent2[0],cent2[1],marker='*')
plt.show()

#计算欧几里得距离
def distEclud(a,b,arrx,arry):
    d1 = arrx-a
    d2 = arry-b
    dist = sqrt( pow(d1,2) + pow(d2,2) )
    return dist

#核心部分
mark = np.zeros((1,m))  #标记矩阵，初始化全为0
for n in range(10):
    #对数据集的每一个点进行标记
    for i in range(m):
        dist1 = distEclud(cent1[0],cent1[1],x[i],y[i])
        dist2 = distEclud(cent2[0],cent2[1],x[i],y[i])
        if dist1 &gt; dist2:
            mark[0][i] = 1   #1表示离质心cent1近，0表示离cent2近
        else:
            mark[0][i] = 0
    
    #更新质心
    sumx1 = sumx2 = sumy1 = sumy2 = 0
    sum_cent1 = sum_cent2 = 0
    for i in range(m):
        if mark[0][i] == 1:
            sumx1 += x[i]
            sumy1 += y[i]
            sum_cent1 += 1
        else:
            sumx2 += x[i]
            sumy2 += y[i]
            sum_cent2 += 1
    ls1 = ls2 = np.zeros((1,2))
    ls1[0][0] = cent1[0]
    ls1[0][1] = cent1[1]
    ls2[0][0] = cent2[0]
    ls2[0][1] = cent2[1]   #为了方便后面对质心的判断
    cent1[0] = sumx1 / sum_cent1
    cent1[1] = sumy1 / sum_cent1
    cent2[0] = sumx2 / sum_cent2
    cent2[1] = sumy2 / sum_cent2
    
    #判断质心是否已经稳定不变
    error_dist1 = distEclud(cent1[0],cent1[1],ls1[0][0],ls1[0][1])
    error_dist2 = distEclud(cent2[0],cent2[1],ls2[0][0],ls2[0][1])
    if error_dist1 &lt;= 0.001 and error_dist2 &lt;= 0.001:
        break

#绘制散点图
col = ['red','green']
for i in range(m):
    if mark[0][i] == 1:
        plt.scatter(x[i],y[i],color=col[0])
    else:
        plt.scatter(x[i],y[i],color=col[1])
plt.scatter(cent1[0],cent1[1],color=col[0],marker='*')
plt.scatter(cent2[0],cent2[1],color=col[1],marker='*')
plt.show()
</code></pre>
<p>运行结果（不唯一）：<br>
<img src="https://JIANG-HS.github.io/post-images/1591348465703.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1591348301269.png" alt="" loading="lazy"></p>
<h1 id="五-k-means-算法存在的问题">五、K-means 算法存在的问题</h1>
<p>由于K-means算法简单且易于实现，因此K-means算法得到了很多的应用，但是从K-means算法的过程中可以发现两个问题：<br>
1.簇中心的个数K是需要事先给定的，所以在处理未知数据时就无从下手。<br>
2.K-means算法在聚类之前，需要随机初始化K个质心，如果质心选择不好，最后的聚类结果可能会比较差。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[神经网络]]></title>
        <id>https://JIANG-HS.github.io/post/1J03ETULG/</id>
        <link href="https://JIANG-HS.github.io/post/1J03ETULG/">
        </link>
        <updated>2020-05-29T11:37:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="一-神经网络的相关描述">一、神经网络的相关描述</h1>
<h2 id="1神经网络">1.神经网络</h2>
<p>人工神经网络（Artificial Neural Network，简写为ANNs），简称<strong>神经网络</strong>（neural network，简写为NN），是一种通过模仿动物神经网络的特征，进行分布式并行信息处理的<strong>算法数学模型</strong>。通过调整大量节点之间相互连接的关系来达到信息处理的目的，拥有自学习和自适应的能力。</p>
<p>一个神经网络结构通常包括输入层、权重、隐层、激活函数、输出层等。</p>
<p>下面是一个隐层为两层的神经网络结构：<br>
<img src="https://JIANG-HS.github.io/post-images/1590803066385.jpg" alt="" loading="lazy"></p>
<p>优点：<br>
第一，具有自学习功能。例如实现图像识别时，只在先把许多不同的图像样板和对应的应识别的结果输入人工神经网络，网络就会通过自学习功能，慢慢学会识别类似的图像。自学习功能对于预测有特别重要的意义。预期未来的人工神经网络计算机将为人类提供经济预测、市场预测、效益预测，其应用前途是很远大的。<br>
第二，具有联想存储功能。用人工神经网络的反馈网络就可以实现这种联想。<br>
第三，具有高速寻找优化解的能力。寻找一个复杂问题的优化解，往往需要很大的计算量，利用一个针对某问题而设计的反馈型人工神经网络，发挥计算机的高速运算能力，可能很快找到优化解。</p>
<h2 id="2激活函数">2.激活函数</h2>
<p>所谓<strong>激活函数</strong>(Activation Function)，就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到下一个神经元或输出端。<br>
<strong>使用激活函数的目的</strong>：如果不使用激活函数，每一层输出都是上一层输入的线性关系，这样就无法对非线性模型进行准确分析；使用激活函数后，就给神经元加入了非线性元素，使得神经网络更加灵活，可以应用在非线性模型中。<br>
<strong>常见的激活函数</strong>：<br>
①Sigmoid函数：也称S型生长曲线，现在的神经网络已经基本不再使用这个函数。函数图像如下：<br>
<img src="https://JIANG-HS.github.io/post-images/1590804552355.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590804559256.jpg" alt="" loading="lazy"><br>
②Tanh函数：是一个双曲正切函数。函数图像如下：<br>
<img src="https://JIANG-HS.github.io/post-images/1590804750456.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590804754119.jpg" alt="" loading="lazy"><br>
③ReLU函数(The Rectified Linear Unit)：现常用于隐层神经元输出。函数图像如下：<br>
<img src="https://JIANG-HS.github.io/post-images/1590805753654.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590805765708.png" alt="" loading="lazy"></p>
<h2 id="3前向传播">3.前向传播</h2>
<p>前向传播就是从输入层到隐层再到输出层的过程，输入层通过计算将数据传递给第一个隐层，再将第一隐层作为输入传递给下一层，直到传到输出层。其中每次传递都需要经过激活函数。</p>
<h2 id="4反向传播">4.反向传播</h2>
<p>每进行了一次前向传播之后，计算输出层与目标函数之间的误差，再将结果代入激活函数的导数计算之后，返回给离输出层最近的隐层，再计算当前隐层与上一层之间的误差，然后逐渐往回传播，直到第一个隐层为止。进行一次反向传播之后，还需要对权重参数进行更新。</p>
<h2 id="5drop-out">5.DROP-OUT</h2>
<p>神经网络是一个全连接操作，当数据过多时，计算速度会很慢，所以当数据量大时通过一个DROP-OUT的方法来提高速率。DROP-OUT时，每一轮前向传播和反向传播都只随机选用部分数据进行操作，但这些部分数据之间依然还是全连接的。<br>
<img src="https://JIANG-HS.github.io/post-images/1590808706033.jpg" alt="" loading="lazy"></p>
<h1 id="二-简单代码实现">二、简单代码实现</h1>
<pre><code>import numpy as np  

#定义激活函数，这里使用到的是Sigmoid函数
def nonlin(x,deriv=False):  
    if(deriv==True):  #定义Sigmoid的导数
        return x*(1-x)  
    return 1/(1+np.exp(-x)) #Sigomid函数

#定义输入数据      
X = np.array([[0,0,1],  
            [0,1,1],  
            [1,0,1],  
            [1,1,1]]) 
#print (X.shape) 

#目标比对模型                  
y = np.array([[0],  
            [1],  
            [1],  
            [0]])  
#print (y.shape)
np.random.seed(1)  

# randomly initialize our weights with mean 0  
w0 = 2*np.random.random((3,4)) - 1  
w1 = 2*np.random.random((4,1)) - 1
#print (w0)
#print (w1)  
#print (w0.shape)
#print (w1.shape)

for j in range(60000):
    #前向传播，l0为输入层，l1为隐层，l2为输出层
    l0 = X  
    l1 = nonlin(np.dot(l0,w0))  #矩阵运算
    l2 = nonlin(np.dot(l1,w1))
    
    l2_error = y - l2  
    #打印误差值
    if (j% 10000) == 0:  
        print (&quot;Error:&quot; + str(np.mean(np.abs(l2_error))))  

    #反向传播          
    l2_delta = l2_error * nonlin(l2,deriv=True)       
    l1_error = l2_delta.dot(w1.T)  
    l1_delta = l1_error * nonlin(l1,deriv=True)  

    #更新权重
    w1 += l1.T.dot(l2_delta)  
    w0 += l0.T.dot(l1_delta)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[梯度下降]]></title>
        <id>https://JIANG-HS.github.io/post/Wj5XKIGfu/</id>
        <link href="https://JIANG-HS.github.io/post/Wj5XKIGfu/">
        </link>
        <updated>2020-05-21T06:21:41.000Z</updated>
        <content type="html"><![CDATA[<h2 id="简介">简介</h2>
<p>梯度下降法（gradient descent）是一个最优化算法，也称为最速下降法（steepest descent）。常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。</p>
<h3 id="核心公式">核心公式</h3>
<figure data-type="image" tabindex="1"><img src="https://JIANG-HS.github.io/post-images/1590110259123.webp" alt="" loading="lazy"></figure>
<p>其中α为步长，也叫学习率。</p>
<h3 id="梯度下降法的代数方式描述">梯度下降法的代数方式描述</h3>
<p>比如对于线性回归，假设函数表示为 h<sub>θ</sub>(x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>)=θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+...+θ<sub>n</sub>x<sub>n</sub>, 其中θ<sub>i</sub> (i = 0,1,2... n)为模型参数，x<sub>i</sub> (i = 0,1,2... n)为每个样本的n个特征值。<br>
损失函数的一般形式为：<br>
<img src="https://JIANG-HS.github.io/post-images/1590135094525.jpg" alt="" loading="lazy"><br>
这里列举双变量的梯度下降：<br>
假如：J(θ)=θ<sup>2</sup><sub>1</sub>+θ<sup>2</sup><sub>2</sub><br>
设初始起点为：θ<sup>0</sup>=(1,3)<br>
初始学习率为：α=0.1<br>
函数的梯度为：▽J(θ)=&lt;2θ<sub>1</sub>,2θ<sub>2</sub>&gt;<br>
进行多次迭代：<br>
<img src="https://JIANG-HS.github.io/post-images/1590136103857.jpg" alt="" loading="lazy"></p>
<h3 id="相关概念">相关概念</h3>
<ul>
<li><strong>损失函数</strong>（Loss Function）：用来描述一个系统在不同参数下的损失，通俗地说就是计算误差。</li>
<li><strong>代价函数</strong>（cost function）：是整个训练集上所有样本误差的平均。本质上看，和损失函数是同一样的。</li>
<li><strong>目标函数</strong>（(objective function）：简单的说，就是你求解后所得出的那个函数。在求解前函数是未知的，按照你的思路将已知条件利用起来，去求解未知量的函数关系式，即为目标函数。</li>
</ul>
<h3 id="缺点">缺点：</h3>
<ul>
<li>靠近极小值时收敛速度减慢；</li>
<li>直线搜索时可能会产生一些问题；</li>
<li>可能会“之字形”地下降。</li>
</ul>
<h2 id="梯度下降三兄弟bgdsgdmbgd">梯度下降三兄弟（BGD，SGD，MBGD）</h2>
<h3 id="1批量梯度下降法batch-gradient-descent">1.批量梯度下降法（Batch Gradient Descent）</h3>
<p>批量梯度下降法每次都使用训练集中的所有样本更新参数。优点：可以得出全局最优解。缺点：样本数据集大时，训练速度慢。</p>
<h3 id="2随机梯度下降法stochastic-gradient-descent">2.随机梯度下降法（Stochastic Gradient Descent）</h3>
<p>随机梯度下降法每次更新都从样本随机选择1组数据。优点：训练速度快。缺点：过程杂乱，准确度下降。</p>
<h3 id="3小批量梯度下降法mini-batch-gradient-descent">3.小批量梯度下降法（Mini-batch Gradient Descent）</h3>
<p>小批量梯度下降法对包含n个样本的数据集进行计算。综合了上述两种方法，既保证了训练速度快，又保证了准确度。</p>
<h2 id="代码实现">代码实现</h2>
<h3 id="1批量梯度下降">1.批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
from random import uniform

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = uniform(0,2)#对theata0随机赋值
theta1 = uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    for i in range(m):
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134239841.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134245318.png" alt="" loading="lazy"></p>
<h3 id="2随机梯度下降">2.随机梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_i = random.randrange(0,m)
    diss = diss + (1/(2*m))*pow((theta0+theta1*x[rand_i]-y[rand_i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p><strong>注意</strong>：因为是随机的，所以每次的结果会不一样。<br>
输出结果：<br>
本次迭代次数为：159次，最终得到theta0=94.02334615793364，theta1=5.968813991236714<br>
本次迭代得到的回归函数是：y=94.02334615793364+5.968813991236714*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134254515.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134259162.png" alt="" loading="lazy"></p>
<h3 id="3小批量梯度下降">3.小批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_ls = list(range(3))
    for i in range(3):
        rand_i = random.randrange(0,m)
        rand_ls[i] = rand_i
    for i in rand_ls:
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134267713.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134271990.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anaconda更新和Spyder更新的指令]]></title>
        <id>https://JIANG-HS.github.io/post/belQdrZZ2/</id>
        <link href="https://JIANG-HS.github.io/post/belQdrZZ2/">
        </link>
        <updated>2020-05-19T14:53:57.000Z</updated>
        <content type="html"><![CDATA[<p>首先打开Anaconda Prompt.exe<br>
1.更新conda(更新Anaconda前需要先更新conda)：conda update conda<br>
2.更新aconda：conda update anaconda<br>
3.更新spyder：conda update spyder</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://JIANG-HS.github.io/post/hello-gridea/</id>
        <link href="https://JIANG-HS.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>