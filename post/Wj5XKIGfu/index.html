<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>梯度下降 | JIANG-HS</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://JIANG-HS.github.io/favicon.ico?v=1593916260860">
<link rel="stylesheet" href="https://JIANG-HS.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="简介
梯度下降法（gradient descent）是一个最优化算法，也称为最速下降法（steepest descent）。常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。
核心公式

其中α为步长，也叫学习率。
梯度下降法的代数..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://JIANG-HS.github.io">
        <img src="https://JIANG-HS.github.io/images/avatar.png?v=1593916260860" class="site-logo">
        <h1 class="site-title">JIANG-HS</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://JIANG-HS.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">梯度下降</h2>
            <div class="post-date">2020-05-21</div>
            
            <div class="post-content" v-pre>
              <h2 id="简介">简介</h2>
<p>梯度下降法（gradient descent）是一个最优化算法，也称为最速下降法（steepest descent）。常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。</p>
<h3 id="核心公式">核心公式</h3>
<figure data-type="image" tabindex="1"><img src="https://JIANG-HS.github.io/post-images/1590110259123.webp" alt="" loading="lazy"></figure>
<p>其中α为步长，也叫学习率。</p>
<h3 id="梯度下降法的代数方式描述">梯度下降法的代数方式描述</h3>
<p>比如对于线性回归，假设函数表示为 h<sub>θ</sub>(x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>)=θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+...+θ<sub>n</sub>x<sub>n</sub>, 其中θ<sub>i</sub> (i = 0,1,2... n)为模型参数，x<sub>i</sub> (i = 0,1,2... n)为每个样本的n个特征值。<br>
损失函数的一般形式为：<br>
<img src="https://JIANG-HS.github.io/post-images/1590135094525.jpg" alt="" loading="lazy"><br>
这里列举双变量的梯度下降：<br>
假如：J(θ)=θ<sup>2</sup><sub>1</sub>+θ<sup>2</sup><sub>2</sub><br>
设初始起点为：θ<sup>0</sup>=(1,3)<br>
初始学习率为：α=0.1<br>
函数的梯度为：▽J(θ)=&lt;2θ<sub>1</sub>,2θ<sub>2</sub>&gt;<br>
进行多次迭代：<br>
<img src="https://JIANG-HS.github.io/post-images/1590136103857.jpg" alt="" loading="lazy"></p>
<h3 id="相关概念">相关概念</h3>
<ul>
<li><strong>损失函数</strong>（Loss Function）：用来描述一个系统在不同参数下的损失，通俗地说就是计算误差。</li>
<li><strong>代价函数</strong>（cost function）：是整个训练集上所有样本误差的平均。本质上看，和损失函数是同一样的。</li>
<li><strong>目标函数</strong>（(objective function）：简单的说，就是你求解后所得出的那个函数。在求解前函数是未知的，按照你的思路将已知条件利用起来，去求解未知量的函数关系式，即为目标函数。</li>
</ul>
<h3 id="缺点">缺点：</h3>
<ul>
<li>靠近极小值时收敛速度减慢；</li>
<li>直线搜索时可能会产生一些问题；</li>
<li>可能会“之字形”地下降。</li>
</ul>
<h2 id="梯度下降三兄弟bgdsgdmbgd">梯度下降三兄弟（BGD，SGD，MBGD）</h2>
<h3 id="1批量梯度下降法batch-gradient-descent">1.批量梯度下降法（Batch Gradient Descent）</h3>
<p>批量梯度下降法每次都使用训练集中的所有样本更新参数。优点：可以得出全局最优解。缺点：样本数据集大时，训练速度慢。</p>
<h3 id="2随机梯度下降法stochastic-gradient-descent">2.随机梯度下降法（Stochastic Gradient Descent）</h3>
<p>随机梯度下降法每次更新都从样本随机选择1组数据。优点：训练速度快。缺点：过程杂乱，准确度下降。</p>
<h3 id="3小批量梯度下降法mini-batch-gradient-descent">3.小批量梯度下降法（Mini-batch Gradient Descent）</h3>
<p>小批量梯度下降法对包含n个样本的数据集进行计算。综合了上述两种方法，既保证了训练速度快，又保证了准确度。</p>
<h2 id="代码实现">代码实现</h2>
<h3 id="1批量梯度下降">1.批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
from random import uniform

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = uniform(0,2)#对theata0随机赋值
theta1 = uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    for i in range(m):
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134239841.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134245318.png" alt="" loading="lazy"></p>
<h3 id="2随机梯度下降">2.随机梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_i = random.randrange(0,m)
    diss = diss + (1/(2*m))*pow((theta0+theta1*x[rand_i]-y[rand_i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p><strong>注意</strong>：因为是随机的，所以每次的结果会不一样。<br>
输出结果：<br>
本次迭代次数为：159次，最终得到theta0=94.02334615793364，theta1=5.968813991236714<br>
本次迭代得到的回归函数是：y=94.02334615793364+5.968813991236714*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134254515.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134259162.png" alt="" loading="lazy"></p>
<h3 id="3小批量梯度下降">3.小批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_ls = list(range(3))
    for i in range(3):
        rand_i = random.randrange(0,m)
        rand_ls[i] = rand_i
    for i in rand_ls:
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134267713.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134271990.png" alt="" loading="lazy"></p>

            </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://JIANG-HS.github.io/post/belQdrZZ2/">
                  <h3 class="post-title">
                    Anaconda更新和Spyder更新的指令
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>





  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: [''],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
