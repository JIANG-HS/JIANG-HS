<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>卷积神经网络 | JIANG-HS</title>
<link rel="shortcut icon" href="https://JIANG-HS.github.io/favicon.ico?v=1596501355780">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://JIANG-HS.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="卷积神经网络 | JIANG-HS - Atom Feed" href="https://JIANG-HS.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="一、卷积神经网络简介
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep lea..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://JIANG-HS.github.io">
  <img class="avatar" src="https://JIANG-HS.github.io/images/avatar.png?v=1596501355780" alt="">
  </a>
  <h1 class="site-title">
    JIANG-HS
  </h1>
  <p class="site-description">
    姜还是老的辣
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              卷积神经网络
            </h2>
            <div class="post-info">
              <span>
                2020-08-02
              </span>
              <span>
                9 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="一-卷积神经网络简介">一、卷积神经网络简介</h1>
<p>卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。卷积神经网络具有表征学习（表征学习指，学习单个符号或一组符号代表什么）能力，能够按其阶层结构对输入信息进行平移不变分类。</p>
<p>对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于计算机视觉、自然语言处理等领域。</p>
<p>卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征，例如像素和音频进行学习、有稳定的效果且对数据没有额外的特征工程（feature engineering）要求。</p>
<h1 id="二-卷积神经网络的结构">二、卷积神经网络的结构</h1>
<p>卷积神经网络的主要结构包括：<strong>输入层（Input Layer）</strong> 、<strong>卷积层（Cony Net Layer）</strong>、<strong>池化层（Pooling Layer）</strong>、<strong>全连接层（Full Connection Layer）</strong> 和  <strong>输出层（Output Layer）</strong> 它是一种前馈式神经网络，每一层都有对应的一种特征输出，并且每个特征图有多个神经元。神经元通过利用对应的滤波器（卷积块或池化块）处理图像所传递过来的信息，构成特征图。每个卷积层后都有一个池化层，从低维映射到高维，此时的映射由于参数过多，维度过高，不适宜作为后层神经的输入，所以必须对该层的输出做降维处理，因此就引入了池化层。若是不对后期数据进行降维，则容易造成过拟合，甚至还会导致维数灾难。</p>
<p>LeCun 最先提出了一个完整的卷积神经网络算法和经典的网络结构模型LeNet-5。下面将采用LeNet-5网络结构来介绍卷积神经网络的结构组成，以及网络的运算法则。其结构如下图所示，当时已成功将其应用于美国银行业的手写字符识别处理中。LeNet-5是第一个产生实际商业价值的卷积神经网络，同时也为卷积神经网络以后的发展奠定了坚实的基础。<br>
<img src="https://JIANG-HS.github.io/post-images/1596349412902.jpg" alt="" loading="lazy"></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>t</mi><mo>−</mo><mn>5</mn><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">结</mi><mi mathvariant="normal">构</mi></mrow><annotation encoding="application/x-tex">LeNet-5网络结构
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">结</span><span class="mord cjk_fallback">构</span></span></span></span></span></p>
<p>下面大致介绍各个网络部分。</p>
<ul>
<li><strong>输入层（INPUT）</strong>：卷积输入层可以直接作用于原始输入数据，对于输入的图像来说，输入数据就是图像的像素值。</li>
<li><strong>卷积层（Convolutions）</strong>：卷积神经网络的卷积层，也叫做特征提取层，包括两个部分。第一部分是真正的卷积层，主要作用是提取输入数据特征。<strong>每一个不同的卷积核提取输入数据的特征都不相同</strong>，卷积层的卷积核数量越多，就能提取越多的输入数据的特征。第二部分是pooling层，也叫下采样层 / <strong>池化层</strong>，主要目的是在保留有用信息的基础上减少数据处理量，加快训练网络的速度。通常情况下，<strong>卷积神经网络至少包含二层卷积层</strong>（这里把真正的卷积层和池化层统称为卷积层），即卷积层，pooling层，卷积层，pooling层。<strong>卷积层数越多，在前一层卷积层基础上就能够提取更加抽象的特征</strong>。</li>
<li><strong>全连接层（Full comection）</strong>：可以包含多个全连接层，实际上就是多层感知机的隐含层部分，通常情况下后面层的神经节点都和前一层的每一个神经节点连接，用一层的神经元节点之间是没有连接的。每一层的神经元节点分别通过连接线上的权值进行前向传播，加权组合得到下一层神经元节点的输入。</li>
<li><strong>输出层（OUTPUT）</strong>：输出层神经节点的数目是根据具体应用任务来设定的。如果是分类任务，卷积神经网络输出层通常是一个分类器。</li>
</ul>
<h1 id="三-卷积神经网络的原理">三、卷积神经网络的原理</h1>
<h2 id="31-卷积层">3.1 卷积层</h2>
<h3 id="311-卷积公式">3.1.1 卷积公式</h3>
<p> 在了解卷积层模型原理之前，我们要先了解什么是卷积。数学中，卷积是两个变量在某范围内相乘后求和的结果。<br>
设：x(t),w(t)是R上的两个可积函数，我们可以得到微积分中卷积的表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mo>∫</mo><mi>x</mi><mo>(</mo><mi>t</mi><mo>−</mo><mi>a</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>a</mi><mo>)</mo><mi>d</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">S(t)=\int x(t-a)w(a)da 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.22225em;vertical-align:-0.86225em;"></span><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;">∫</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span></span></span></span></span></p>
<p>如果卷积的变量是序列x(t)和w(t)（即卷积的离散形式），则卷积的表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>a</mi></munder><mi>x</mi><mo>(</mo><mi>t</mi><mo>−</mo><mi>a</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(t)=\sum_{a}x(t-a)w(a) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mclose">)</span></span></span></span></span></p>
<p>上面两个式子都可以表示为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>t</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(t)=(X*W)(t) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span></p>
<p>式子中的星号*表示卷积。<br>
我们再把卷积公式推广到二维卷积，则表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>m</mi></munder><munder><mo>∑</mo><mi>n</mi></munder><mi>x</mi><mo>(</mo><mi>i</mi><mo>−</mo><mi>m</mi><mo separator="true">,</mo><mi>j</mi><mo>−</mo><mi>n</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(i,j)=(X*W)(i,j)=\sum_{m}\sum_{n}x(i-m,j-n)w(m,n) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span></p>
<p> 至此，我们已经大致的了解了什么是卷积，但是CNN中的卷积层用到的卷积公式和数学中的稍有不同，比如对于二维的卷积，CNN中的公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>m</mi></munder><munder><mo>∑</mo><mi>n</mi></munder><mi>x</mi><mo>(</mo><mi>i</mi><mo>+</mo><mi>m</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>n</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(i,j)=(X*W)(i,j)=\sum_{m}\sum_{n}x(i+m,j+n)w(m,n) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span></p>
<p> 其中，W称为<strong>卷积核</strong>，而X则称为输入。W和X的维度是相通的，比如：如果X是一个二维码输入的矩阵，则W也是一个二维的矩阵；如果X是多为张量，则W也是一个多维的张量。</p>
<h3 id="312-卷积层工作原理">3.1.2 卷积层工作原理</h3>
<p> 不同的卷积核能够提取到图像中的不同特征，卷积运算的一个重要的特点就是：通过即卷积运算，可以使原信号特征增强，并且<strong>降低噪音</strong>。<br>
 卷积核在二维平面上移动，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和，通过卷积核的不断移动，就有了一个新的图像，这个图像完全有卷积核在各个位置时的乘积求和结果组成。<br>
 二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是卷积核的大小）加权求和得到该像素点的输出值。具体做法如下图所示（图中，输入的图像大小为5*5，有一个3*3大小的滤波器（filter）在移动，输出的是一个3*3的特征图(Feature Map)）：<br>
<img src="https://JIANG-HS.github.io/post-images/1596363685613.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1596362332207.gif" alt="" loading="lazy"></p>
<p> 事实上，卷积网络中的卷积核参数是通过网络训练学出的，除了可以学到类似的横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。当然，不仅如此，检测颜色、形状、纹理等众多基本模式的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。通过“组合”这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，并以此对应到具体的样本类别中。</p>
<p> 在经典的LeNet网络中，以CI层进行说明：C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5*5，能够输出6个特征图，大小为28*28。C1有156个可训练参数(每个滤波器有5*5=25个unit参数和一个bias参数，一共6个滤波器，共(5*5+1)*6=156个参数)，共156*(28*28)=122304个连接。</p>
<h2 id="32-池化层">3.2 池化层</h2>
<p>本节将介绍池化层，通常使用的池化操作为平均值池化(average-pooling) 和最大值池化(max-pooling)。需要指出的是，同卷积层操作不同，池化层不包含需要学习的参数，使用时仅需指定池化类型(average或may)、池化是作的慈大小(kemel size)和池化援作的步长等超参数即可。作为池化结果，即： 平方值(最大值)池化在每次操作时，将池化核覆盖区域中所有值的平均值(最大值)<br>
最大池化结果为：</p>
<h2 id="33-激活函数">3.3 激活函数</h2>
<h2 id="34-全连接层">3.4 全连接层</h2>
<h2 id="35-反馈运算反向传播">3.5 反馈运算（反向传播）</h2>
<h1 id="四-卷积神经网络的优点">四、卷积神经网络的优点</h1>
<h2 id="41-图像特征的层次化结构">4.1 图像特征的层次化结构</h2>
<h2 id="42-卷积神经网络的仿生物学理论">4.2 卷积神经网络的仿生物学理论</h2>
<h2 id="43-卷积神经网络的局部连接属性">4.3 卷积神经网络的局部连接属性</h2>
<h2 id="44-卷积神经网络的权值共享特征">4.4 卷积神经网络的权值共享特征</h2>
<h2 id="45-卷积神经网络端对端的处理方式">4.5 卷积神经网络端对端的处理方式</h2>
<h1 id="五-代码实现">五、代码实现</h1>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E4%B8%80-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B">一、卷积神经网络简介</a></li>
<li><a href="#%E4%BA%8C-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84">二、卷积神经网络的结构</a></li>
<li><a href="#%E4%B8%89-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86">三、卷积神经网络的原理</a>
<ul>
<li><a href="#31-%E5%8D%B7%E7%A7%AF%E5%B1%82">3.1 卷积层</a>
<ul>
<li><a href="#311-%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F">3.1.1 卷积公式</a></li>
<li><a href="#312-%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86">3.1.2 卷积层工作原理</a></li>
</ul>
</li>
<li><a href="#32-%E6%B1%A0%E5%8C%96%E5%B1%82">3.2 池化层</a></li>
<li><a href="#33-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">3.3 激活函数</a></li>
<li><a href="#34-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82">3.4 全连接层</a></li>
<li><a href="#35-%E5%8F%8D%E9%A6%88%E8%BF%90%E7%AE%97%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.5 反馈运算（反向传播）</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9B-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E7%82%B9">四、卷积神经网络的优点</a>
<ul>
<li><a href="#41-%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E7%BB%93%E6%9E%84">4.1 图像特征的层次化结构</a></li>
<li><a href="#42-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%BF%E7%94%9F%E7%89%A9%E5%AD%A6%E7%90%86%E8%AE%BA">4.2 卷积神经网络的仿生物学理论</a></li>
<li><a href="#43-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5%E5%B1%9E%E6%80%A7">4.3 卷积神经网络的局部连接属性</a></li>
<li><a href="#44-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB%E7%89%B9%E5%BE%81">4.4 卷积神经网络的权值共享特征</a></li>
<li><a href="#45-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AB%AF%E5%AF%B9%E7%AB%AF%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F">4.5 卷积神经网络端对端的处理方式</a></li>
</ul>
</li>
<li><a href="#%E4%BA%94-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">五、代码实现</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://JIANG-HS.github.io/post/1J03ETULG/">
              <h3 class="post-title">
                神经网络
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '',
    clientSecret: '',
    repo: '',
    owner: '',
    admin: [''],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://JIANG-HS.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
