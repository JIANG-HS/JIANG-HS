<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>卷积神经网络 | JIANG-HS</title>
<link rel="shortcut icon" href="https://JIANG-HS.github.io/favicon.ico?v=1600693149942">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://JIANG-HS.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="卷积神经网络 | JIANG-HS - Atom Feed" href="https://JIANG-HS.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-xxxxxxxxx-x"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-xxxxxxxxx-x');
</script>


    <meta name="description" content="一、卷积神经网络简介
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep lea..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://JIANG-HS.github.io">
  <img class="avatar" src="https://JIANG-HS.github.io/images/avatar.png?v=1600693149942" alt="">
  </a>
  <h1 class="site-title">
    JIANG-HS
  </h1>
  <p class="site-description">
    姜还是老的辣
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              卷积神经网络
            </h2>
            <div class="post-info">
              <span>
                2020-08-02
              </span>
              <span>
                18 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="一-卷积神经网络简介">一、卷积神经网络简介</h1>
<p>卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。卷积神经网络具有表征学习（表征学习指，学习单个符号或一组符号代表什么）能力，能够按其阶层结构对输入信息进行平移不变分类。</p>
<p>对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于计算机视觉、自然语言处理等领域。</p>
<p>卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征，例如像素和音频进行学习、有稳定的效果且对数据没有额外的特征工程（feature engineering）要求。</p>
<h1 id="二-卷积神经网络的结构">二、卷积神经网络的结构</h1>
<p>卷积神经网络的主要结构包括：<strong>输入层（Input Layer）</strong> 、<strong>卷积层（Cony Net Layer）</strong>、<strong>池化层（Pooling Layer）</strong>、<strong>全连接层（Full Connection Layer）</strong> 和  <strong>输出层（Output Layer）</strong> 。它是一种前馈式神经网络，每一层都有对应的一种特征输出，并且每个特征图有多个神经元。神经元通过利用对应的滤波器（卷积块或池化块）处理图像所传递过来的信息，构成特征图。每个卷积层后都有一个池化层，从低维映射到高维，此时的映射由于参数过多，维度过高，不适宜作为后层神经的输入，所以必须对该层的输出做降维处理，因此就引入了池化层。若是不对后期数据进行降维，则容易造成过拟合，甚至还会导致维数灾难。</p>
<p>LeCun 最先提出了一个完整的卷积神经网络算法和经典的网络结构模型LeNet-5。下面将采用LeNet-5网络结构来介绍卷积神经网络的结构组成，以及网络的运算法则。其结构如下图所示，当时已成功将其应用于美国银行业的手写字符识别处理中。LeNet-5是第一个产生实际商业价值的卷积神经网络，同时也为卷积神经网络以后的发展奠定了坚实的基础。<br>
<img src="https://JIANG-HS.github.io/post-images/1596349412902.jpg" alt="" loading="lazy"></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>t</mi><mo>−</mo><mn>5</mn><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">结</mi><mi mathvariant="normal">构</mi></mrow><annotation encoding="application/x-tex">LeNet-5网络结构
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">结</span><span class="mord cjk_fallback">构</span></span></span></span></span></p>
<p>下面大致介绍各个网络部分。</p>
<ul>
<li><strong>输入层（INPUT）</strong>：卷积输入层可以直接作用于原始输入数据，对于输入的图像来说，输入数据就是图像的像素值。</li>
<li><strong>卷积层（Convolutions）</strong>：卷积神经网络的卷积层，也叫做特征提取层，包括两个部分。第一部分是真正的卷积层，主要作用是提取输入数据特征。<strong>每一个不同的卷积核提取输入数据的特征都不相同</strong>，卷积层的卷积核数量越多，就能提取越多的输入数据的特征。第二部分是pooling层，也叫下采样层 / <strong>池化层</strong>，主要目的是在保留有用信息的基础上减少数据处理量，加快训练网络的速度。通常情况下，<strong>卷积神经网络至少包含二层卷积层</strong>（这里把真正的卷积层和池化层统称为卷积层），即卷积层，pooling层，卷积层，pooling层。<strong>卷积层数越多，在前一层卷积层基础上就能够提取更加抽象的特征</strong>。</li>
<li><strong>全连接层（Full comection）</strong>：可以包含多个全连接层，实际上就是多层感知机的隐含层部分，通常情况下后面层的神经节点都和前一层的每一个神经节点连接，用一层的神经元节点之间是没有连接的。每一层的神经元节点分别通过连接线上的权值进行前向传播，加权组合得到下一层神经元节点的输入。</li>
<li><strong>输出层（OUTPUT）</strong>：输出层神经节点的数目是根据具体应用任务来设定的。如果是分类任务，卷积神经网络输出层通常是一个分类器。</li>
</ul>
<h1 id="三-卷积神经网络的原理">三、卷积神经网络的原理</h1>
<h2 id="31-卷积层">3.1 卷积层</h2>
<h3 id="311-卷积公式">3.1.1 卷积公式</h3>
<p> 在了解卷积层模型原理之前，我们要先了解什么是卷积。数学中，卷积是两个变量在某范围内相乘后求和的结果。<br>
设：x(t),w(t)是R上的两个可积函数，我们可以得到微积分中卷积的表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mo>∫</mo><mi>x</mi><mo>(</mo><mi>t</mi><mo>−</mo><mi>a</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>a</mi><mo>)</mo><mi>d</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">S(t)=\int x(t-a)w(a)da 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.22225em;vertical-align:-0.86225em;"></span><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;">∫</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span></span></span></span></span></p>
<p>如果卷积的变量是序列x(t)和w(t)（即卷积的离散形式），则卷积的表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>a</mi></munder><mi>x</mi><mo>(</mo><mi>t</mi><mo>−</mo><mi>a</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>a</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(t)=\sum_{a}x(t-a)w(a) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mclose">)</span></span></span></span></span></p>
<p>上面两个式子都可以表示为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>t</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(t)=(X*W)(t) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span></p>
<p>式子中的星号*表示卷积。<br>
我们再把卷积公式推广到二维卷积，则表达式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>m</mi></munder><munder><mo>∑</mo><mi>n</mi></munder><mi>x</mi><mo>(</mo><mi>i</mi><mo>−</mo><mi>m</mi><mo separator="true">,</mo><mi>j</mi><mo>−</mo><mi>n</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(i,j)=(X*W)(i,j)=\sum_{m}\sum_{n}x(i-m,j-n)w(m,n) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span></p>
<p> 至此，我们已经大致的了解了什么是卷积，但是CNN中的卷积层用到的卷积公式和数学中的稍有不同，比如对于二维的卷积，CNN中的公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>X</mi><mo>∗</mo><mi>W</mi><mo>)</mo><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mi>m</mi></munder><munder><mo>∑</mo><mi>n</mi></munder><mi>x</mi><mo>(</mo><mi>i</mi><mo>+</mo><mi>m</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>n</mi><mo>)</mo><mi>w</mi><mo>(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">s(i,j)=(X*W)(i,j)=\sum_{m}\sum_{n}x(i+m,j+n)w(m,n) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3000100000000003em;vertical-align:-1.250005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8999949999999999em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.250005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span></p>
<p> 其中，W称为<strong>卷积核</strong>，而X则称为输入。W和X的维度是相通的，比如：如果X是一个二维码输入的矩阵，则W也是一个二维的矩阵；如果X是多为张量，则W也是一个多维的张量。</p>
<h3 id="312-卷积层工作原理">3.1.2 卷积层工作原理</h3>
<p> 不同的卷积核能够提取到图像中的不同特征，卷积运算的一个重要的特点就是：通过即卷积运算，可以使原信号特征增强，并且<strong>降低噪音</strong>。<br>
 卷积核在二维平面上移动，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和，通过卷积核的不断移动，就有了一个新的图像，这个图像完全有卷积核在各个位置时的乘积求和结果组成。<br>
 二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是卷积核的大小）加权求和得到该像素点的输出值。具体做法如下图所示（图中，输入的图像大小为5*5，有一个3*3大小的滤波器（filter）在移动，输出的是一个3*3的特征图(Feature Map)）：<br>
<img src="https://JIANG-HS.github.io/post-images/1596363685613.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1596362332207.gif" alt="" loading="lazy"></p>
<p> 为了增加网络的表达能力，卷积层的输出结果往往还要经过激活函数，即把得到的特征图中的数据带入到激活函数中，得到一个新的矩阵，这个矩阵就是卷积层真真的输出。<br>
 事实上，卷积网络中的卷积核参数是通过网络训练学出的，除了可以学到类似的横向、纵向边缘滤波器，还可以学到任意角度的边缘滤波器。当然，不仅如此，检测颜色、形状、纹理等众多基本模式的滤波器（卷积核）都可以包含在一个足够复杂的深层卷积神经网络中。通过“组合”这些滤波器（卷积核）以及随着网络后续操作的进行，基本而一般的模式会逐渐被抽象为具有高层语义的“概念”表示，并以此对应到具体的样本类别中。</p>
<p> 在经典的LeNet网络中，以CI层进行说明：C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5*5，能够输出6个特征图，大小为28*28。C1有156个可训练参数(每个滤波器有5*5=25个unit参数和一个bias参数，一共6个滤波器，共(5*5+1)*6=156个参数)，共156*(28*28)=122304个连接。</p>
<h2 id="32-池化层">3.2 池化层</h2>
<p> 下面将介绍池化层，池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。<br>
 通常使用的池化操作为<strong>平均值池化（average-pooling）</strong> 和<strong>最大值池化（max-pooling）</strong>。需要指出的是，同卷积层操作不同，池化层不包含需要学习的参数，使用时仅需指定<strong>池化类型</strong>（average 或 max）、<strong>池化操作的核大小</strong>（kernel size） 和 <strong>池化操作的步长</strong>（stride）等超参数即可。平均值（最大值）池化在每次操作时，将池化核覆盖区域中所有值的平均值（最大值）作为池化结果。</p>
<p> 接下来我们就举例说明池化层到底是如何实现的。下图中，池化核的大小为2*2，池化操作的步长为2。<br>
<img src="https://JIANG-HS.github.io/post-images/1596524052019.jpeg" alt="" width="500" loading="lazy"><br>
 首先对黄色（左上角1,3,1,3）的2*2区域进行池化，由于此2×2区域的平均值为2（最大值为3），那么对应的池化输出位置的值也为2（3），由于步幅为2，此时移动到红色（右上角2,4,2,4）的位置去进行池化，输出的平均值为3（最大值为4）。同样的方法，可以得到蓝色区域（左下角5,7,5,7）和 橙色区域（右下角6,8,6,8）的输出值。最终，输入4*4的矩阵在池化后变成了2*2的矩阵，进行了压缩。</p>
<p> 在上面的例子中可以发现，池化操作后的结果相比其输入变了，其实池化操作实际上就是一种“<strong>下采样</strong>”操作。另一方面，池化也可以看成是一个用p范数作为非线性映射的“卷积”操作，特别的，当p趋近正无穷时就是最常见的最大值池化。<br>
池化层的引入是仿照人的视觉系统对视觉输入对象进行降维（下采样）和抽象。在卷积神经网络过去的工作中，研究者普遍认为池化层有如下3种功效：</p>
<ul>
<li><strong>特征不变性</strong>：池化操作使模型更关注是否存在某些特征而不是特征具体的位置，可看做是一种很强的先验，使特征学习包含某种强度的自由度，能容忍一些特征微小的位移。</li>
<li><strong>特征降维</strong>：由于池化操作的下采样作用，池化结果中的一个元素对应于原输入数据的一个子区域，因此池化相当于在空间范围内做了维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。</li>
<li>在一定程度<strong>防止过拟合</strong>，更方便优化。</li>
</ul>
<p> 拓展：池化操作并不是卷积神经网络必须的元件或操作。近期，德国著名高校弗赖堡大学的研究者提出用一种特殊的卷积操作来代替池化层实现降采样，进而构建一个只含卷积操作的网络，其实验结果显示这种改造的网络可以达到甚至超过传统卷积神经网络（卷积层池化层交替）的分类精度。</p>
<h2 id="33-全连接层">3.3 全连接层</h2>
<p> 全连接层（Fully Connected layers,FC）的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。<br>
 全连接层在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐藏层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在基本的CNN网络中，全连接层的作用是将经过多个卷积层和池化层的图像特征图中的特征进行整合，获取图像特征具有的高层含义，之后用于图像分类。在CNN网络中，全连接层将卷积层产生的特征图映射成一个固定长度（一般为输入图像数据集中的图像类别数）的特征向量。这个特征向量包含了输入图像所有特征的组合信息，虽然丢失了图像的位置信息，但是该向量将图像中最具有特点的图像特征保留了下来，以此完成图像分类任务。<br>
 下图中连线最密集的2个地方就是全连接层，这很明显的可以看出全连接层的参数的确很多。在前向计算过程，也就是一个线性的加权求和的过程，全连接层的每一个输出都可以看成前一层的每一个结点乘以一个权重系数W，最后加上一个偏置值b得到，即 。如下图中第一个全连接层，输入有50*4*4个神经元结点，输出有500个结点，则一共需要50*4*4*500=400000个权值参数W和500个偏置参数b。<br>
<img src="https://JIANG-HS.github.io/post-images/1596528537976.jpg" alt="" loading="lazy"></p>
<h2 id="34-反向传播">3.4 反向传播</h2>
<h1 id="四-卷积神经网络的优点">四、卷积神经网络的优点</h1>
<h2 id="41-图像特征的层次化结构">4.1 图像特征的层次化结构</h2>
<p> 图像特征具有层次化结构，传统方法无法做到自动组合这些层次化的特征，图像特征的层次化结构如下图所示：<br>
<img src="https://JIANG-HS.github.io/post-images/1596534656283.jpg" alt="" loading="lazy"><br>
 图像相邻区域的像素组成边缘线条，边缘线条组合得到图像纹理，图像纹理经过组合形成局部图案，最后所有的局部图案构成一个图像物体。卷积神经网络具有组合图像层次化特征的能力。</p>
<h2 id="42-卷积神经网络的仿生物学理论">4.2 卷积神经网络的仿生物学理论</h2>
<p> 1981年，David Hubel和Torsten Wiesel在猫的大脑视觉皮层上所做的实验证明了人类大脑视觉系统其实是不断地将低级特征通过神经元之间的连线传递为高级特征的过程，通过组合低层特征一步一步得到高层特征，越是高层特征就变得越抽象。卷积神经网络卷积层的二维卷积方式使其能够直接从图像像素中提取数据特征，这种处理方式更加接近人类大脑视觉系统的工作方式。</p>
<h2 id="43-卷积神经网络的局部连接属性">4.3 卷积神经网络的局部连接属性</h2>
<p> 卷积神经网络属于局部连接网络，局部连接网络是基于对自然图像的深刻研究而提出来的。由于自然图像存在局部区域稳定性的属性，自然图像中某一局部区域的统计特征相对于图像的其他相邻局部区域具有相似性，因此<strong>神经网络从自然图像中学习到的某一局部区域的特征同样适合于图像的其他相邻局部区域</strong>。下图中，左边为全连接网络，右边为局部连接网络。<br>
<img src="https://JIANG-HS.github.io/post-images/1596535462312.jpg" alt="" loading="lazy"><br>
 局部连接网络比全连接网络有很大的优势。假设左边的输入图像层为L1层，右边神经节点层为L2层，对于图中的全连接网络层来说，如果L1层输入图像的分辨率为1000×1000，L2隐含层有100万个神经元，每个隐含层神经元全部都连接到LI层输入图像的每一个像素点，那么连接线达到了1000×1000×1000000=10^12，也就是10的12次方个权值参数。对于图中的局部连接网络，L2隐含层中每一个神经节点与L1层节点相同位置附近10×10大小的图像区域相连接，则100万个隐层神经元只有100万×100，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">10^{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>个权值参数，其权值连接个数比全连接网络足足减少了4个数量级。因此卷积神经网络相对于全连接网络来说，在训练速度上有很大的优势。</p>
<h2 id="44-卷积神经网络的权值共享特征">4.4 卷积神经网络的权值共享特征</h2>
<p> 权值共享是卷积神经网络的另外一个重要特性，卷积神经网络中<strong>相同的卷积核共享相同的卷积核权值和偏置值</strong>。同一种卷积核使用同样的权值按照某种顺序去卷积图像，比如从左到右，从上到下的顺序进行图像卷积。那么卷积后得到所有神经节点都是共享连接参数，也就是说每个神经元都是用同一个卷积核去卷积图像。所以一种卷积核只提取了图像的一种特征，如果需要提取多种输入图像的不同特征，则需要使用多种卷积核。<strong>权值共享减少了卷积神经网络需要学习的参数</strong>。<br>
 下面的动态图演示很好地体现了这一特征：<br>
<img src="https://JIANG-HS.github.io/post-images/1596536088417.gif" alt="" loading="lazy"></p>
<h2 id="45-卷积神经网络端对端的处理方式">4.5 卷积神经网络端对端的处理方式</h2>
<p> 传统图像分类中，研究人员需要花费大量精力去研究如何提取到更好的图像特征，如HOG和SIFT等特征。卷积神经网络进行图像分类的最大优点是采用端对端的处理方式，把传统图像分类任务中的图像预处理、特征提取变为一个黑盒子，研究人员只需要把精力放在研究如何设计卷积神经网络的网络架构和优化网络参数上。卷积神经网络把与图像卷积得到的特征进行前向传播，然后通过网络输出值与数据标签的差值反向传播来调整网络参数，通过这样的方式卷积神经网络能够自动提取到有利于分类任务的特征，不需要人为干预。下图展示了卷积神经网络与传统图像分类之间的差异性。<br>
<img src="https://JIANG-HS.github.io/post-images/1596535081987.jpg" alt="" loading="lazy"><br>
 传统图像分类任务中的图像预处理，图像特征提取的每一个处理步骤需要非常专业的图像处理知识才能很好地完成整个图像分类任务。卷积神经网络大大简化了图像分类任务的流程，使用者不需要知道如何进行图像特征提取也能出色地完成图像分类任务。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E4%B8%80-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B">一、卷积神经网络简介</a></li>
<li><a href="#%E4%BA%8C-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84">二、卷积神经网络的结构</a></li>
<li><a href="#%E4%B8%89-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%9F%E7%90%86">三、卷积神经网络的原理</a>
<ul>
<li><a href="#31-%E5%8D%B7%E7%A7%AF%E5%B1%82">3.1 卷积层</a>
<ul>
<li><a href="#311-%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F">3.1.1 卷积公式</a></li>
<li><a href="#312-%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86">3.1.2 卷积层工作原理</a></li>
</ul>
</li>
<li><a href="#32-%E6%B1%A0%E5%8C%96%E5%B1%82">3.2 池化层</a></li>
<li><a href="#33-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82">3.3 全连接层</a></li>
<li><a href="#34-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.4 反向传播</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9B-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E7%82%B9">四、卷积神经网络的优点</a>
<ul>
<li><a href="#41-%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E7%BB%93%E6%9E%84">4.1 图像特征的层次化结构</a></li>
<li><a href="#42-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%BF%E7%94%9F%E7%89%A9%E5%AD%A6%E7%90%86%E8%AE%BA">4.2 卷积神经网络的仿生物学理论</a></li>
<li><a href="#43-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%80%E9%83%A8%E8%BF%9E%E6%8E%A5%E5%B1%9E%E6%80%A7">4.3 卷积神经网络的局部连接属性</a></li>
<li><a href="#44-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB%E7%89%B9%E5%BE%81">4.4 卷积神经网络的权值共享特征</a></li>
<li><a href="#45-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AB%AF%E5%AF%B9%E7%AB%AF%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F">4.5 卷积神经网络端对端的处理方式</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://JIANG-HS.github.io/post/1J03ETULG/">
              <h3 class="post-title">
                神经网络
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '',
    clientSecret: '',
    repo: '',
    owner: '',
    admin: [''],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://JIANG-HS.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
