<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://JIANG-HS.github.io</id>
    <title>JIANG-HS</title>
    <updated>2020-05-22T10:12:19.088Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://JIANG-HS.github.io"/>
    <link rel="self" href="https://JIANG-HS.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://JIANG-HS.github.io/images/avatar.png</logo>
    <icon>https://JIANG-HS.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, JIANG-HS</rights>
    <entry>
        <title type="html"><![CDATA[梯度下降]]></title>
        <id>https://JIANG-HS.github.io/post/Wj5XKIGfu/</id>
        <link href="https://JIANG-HS.github.io/post/Wj5XKIGfu/">
        </link>
        <updated>2020-05-21T06:21:41.000Z</updated>
        <content type="html"><![CDATA[<h2 id="简介">简介</h2>
<p>梯度下降法（gradient descent）是一个最优化算法，也称为最速下降法（steepest descent）。常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。</p>
<h3 id="核心公式">核心公式</h3>
<figure data-type="image" tabindex="1"><img src="https://JIANG-HS.github.io/post-images/1590110259123.webp" alt="" loading="lazy"></figure>
<p>其中α为步长，也叫学习率。</p>
<h3 id="梯度下降法的代数方式描述">梯度下降法的代数方式描述</h3>
<p>比如对于线性回归，假设函数表示为 h<sub>θ</sub>(x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>)=θ<sub>0</sub>+θ<sub>1</sub>x<sub>1</sub>+...+θ<sub>n</sub>x<sub>n</sub>, 其中θ<sub>i</sub> (i = 0,1,2... n)为模型参数，x<sub>i</sub> (i = 0,1,2... n)为每个样本的n个特征值。<br>
损失函数的一般形式为：<br>
<img src="https://JIANG-HS.github.io/post-images/1590135094525.jpg" alt="" loading="lazy"><br>
这里列举双变量的梯度下降：<br>
假如：J(θ)=θ<sup>2</sup><sub>1</sub>+θ<sup>2</sup><sub>2</sub><br>
设初始起点为：θ<sup>0</sup>=(1,3)<br>
初始学习率为：α=0.1<br>
函数的梯度为：▽J(θ)=&lt;2θ<sub>1</sub>,2θ<sub>2</sub>&gt;<br>
进行多次迭代：<br>
<img src="https://JIANG-HS.github.io/post-images/1590136103857.jpg" alt="" loading="lazy"></p>
<h3 id="相关概念">相关概念</h3>
<ul>
<li><strong>损失函数</strong>（Loss Function）：用来描述一个系统在不同参数下的损失，通俗地说就是计算误差。</li>
<li><strong>代价函数</strong>（cost function）：是整个训练集上所有样本误差的平均。本质上看，和损失函数是同一样的。</li>
<li><strong>目标函数</strong>（(objective function）：简单的说，就是你求解后所得出的那个函数。在求解前函数是未知的，按照你的思路将已知条件利用起来，去求解未知量的函数关系式，即为目标函数。</li>
</ul>
<h3 id="缺点">缺点：</h3>
<ul>
<li>靠近极小值时收敛速度减慢；</li>
<li>直线搜索时可能会产生一些问题；</li>
<li>可能会“之字形”地下降。</li>
</ul>
<h2 id="梯度下降三兄弟bgdsgdmbgd">梯度下降三兄弟（BGD，SGD，MBGD）</h2>
<h3 id="1批量梯度下降法batch-gradient-descent">1.批量梯度下降法（Batch Gradient Descent）</h3>
<p>批量梯度下降法每次都使用训练集中的所有样本更新参数。优点：可以得出全局最优解。缺点：样本数据集大时，训练速度慢。</p>
<h3 id="2随机梯度下降法stochastic-gradient-descent">2.随机梯度下降法（Stochastic Gradient Descent）</h3>
<p>随机梯度下降法每次更新都从样本随机选择1组数据。优点：训练速度快。缺点：过程杂乱，准确度下降。</p>
<h3 id="3小批量梯度下降法mini-batch-gradient-descent">3.小批量梯度下降法（Mini-batch Gradient Descent）</h3>
<p>小批量梯度下降法对包含n个样本的数据集进行计算。综合了上述两种方法，既保证了训练速度快，又保证了准确度。</p>
<h2 id="代码实现">代码实现</h2>
<h3 id="1批量梯度下降">1.批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
from random import uniform

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = uniform(0,2)#对theata0随机赋值
theta1 = uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    for i in range(m):
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134239841.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134245318.png" alt="" loading="lazy"></p>
<h3 id="2随机梯度下降">2.随机梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_i = random.randrange(0,m)
    diss = diss + (1/(2*m))*pow((theta0+theta1*x[rand_i]-y[rand_i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p><strong>注意</strong>：因为是随机的，所以每次的结果会不一样。<br>
输出结果：<br>
本次迭代次数为：159次，最终得到theta0=94.02334615793364，theta1=5.968813991236714<br>
本次迭代得到的回归函数是：y=94.02334615793364+5.968813991236714*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134254515.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134259162.png" alt="" loading="lazy"></p>
<h3 id="3小批量梯度下降">3.小批量梯度下降</h3>
<pre><code>import matplotlib.pyplot as plt
import matplotlib
from math import pow
import random

x = [1,3,5,7,9,11,13]
y = [100,111,130,144,149,166,179]

#目标函数为 y=theta0+theta1*x
#参数定义
theta0 = random.uniform(0,2)#对theata0随机赋值
theta1 = random.uniform(0,2)#对theata1随机赋值
alpha = 0.1#学习率
m = len(x)

count = 0
loss = []

for num in range(10000):
    count += 1
    diss = 0   #误差
    deriv0 = 0 #导数
    deriv1 = 0 #导数
    for i in range(m):
        deriv0 += (theta0+theta1*x[i]-y[i])/m
        deriv1 += ((theta0+theta1*x[i]-y[i])/m)*x[i]

    #更新theta0和theta1
    for i in range(m):
        theta0 = theta0 - alpha*((theta0+theta1*x[i]-y[i])/m) 
        theta1 = theta1 - alpha*((theta0+theta1*x[i]-y[i])/m)*x[i]

    #求损失函数J(θ)
    rand_ls = list(range(3))
    for i in range(3):
        rand_i = random.randrange(0,m)
        rand_ls[i] = rand_i
    for i in rand_ls:
        diss = diss + (1/(2*m))*pow((theta0+theta1*x[i]-y[i]),2)
    loss.append(diss)

    #如果误差已经很小，则退出循环
    if diss &lt;= 0.001:
        break
    
print(&quot;本次迭代次数为：{}次，最终得到theta0={}，theta1={}&quot;.format(count,theta0,theta1))
print(&quot;本次迭代得到的回归函数是：y={}+{}*x&quot;.format(theta0,theta1))
#画原始数据图和目标函数图
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
plt.plot(x,y,'bo',label='数据')
plt.plot(x,[theta0+theta1*x for x in x],label='目标函数')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#画损失函数（误差）变化图
plt.scatter(range(count),loss)
plt.show()
</code></pre>
<p>输出结果：<br>
本次迭代次数为：10000次，最终得到theta0=96.86808644268262，theta1=5.762687172041142<br>
本次迭代得到的回归函数是：y=96.86808644268262+5.762687172041142*x<br>
<img src="https://JIANG-HS.github.io/post-images/1590134267713.png" alt="" loading="lazy"><br>
<img src="https://JIANG-HS.github.io/post-images/1590134271990.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anaconda更新和Spyder更新的指令]]></title>
        <id>https://JIANG-HS.github.io/post/belQdrZZ2/</id>
        <link href="https://JIANG-HS.github.io/post/belQdrZZ2/">
        </link>
        <updated>2020-05-19T14:53:57.000Z</updated>
        <content type="html"><![CDATA[<p>首先打开Anaconda Prompt.exe<br>
1.更新conda(更新Anaconda前需要先更新conda)：conda update conda<br>
2.更新aconda：conda update anaconda<br>
3.更新spyder：conda update spyder</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://JIANG-HS.github.io/post/hello-gridea/</id>
        <link href="https://JIANG-HS.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>